# OctFormer: Octree-based Transformers for 3D Point Clouds

## 0 Abstraction

OctFormer 目标是 3D 点云的理解问题，可以作为主干网络，而且计算复杂度随输入变化是线性的。作者观察到注意力机制并不在意局部窗口的形状，因此他利用了八叉树的排序打乱键来将点云划分为**包含相同数量点**的局部窗口，同时允许窗口形状自由变化。作者还引入了扩展的八叉树注意力机制以进一步扩展感受野。

## 1 Introduction

Transformer-based 点云理解问题当前的难点：
- 和 CNN 在较大的数据集上效果接近了，但是计算量太大了。
- 利用局部窗口减小计算量，但是局部窗口内点的数量变化很大，影响 GPU 效率。
- 将点云抽象成降采样的特征图又降低了网络的能力。

作者观察到
- 注意力机制并不在意局部窗口的形状，因此局部窗口不必是相同大小的立方体，并且能控制不同的局部窗口内点的数量相近。因此可以直接用 PyTorch 实现，而不是自定义 CUDA 算子。
- 用并行算法构建的八叉树节点已经排列成 Z 型顺序了，因此空间中相近的点在内存中也比较相近。作者按照八叉树节点的顺序将特征存储在张量中。在每个窗口中，作者填充了一些零值，以使张量的空间尺寸能够被每个窗口中指定的点数整除，然后可以通过简单地调整张量的形状来高效地生成窗口划分，几乎不增加成本。
- 为了进一步增加 OctFormer 的感受野，作者引入了一种带有空间维度上膨胀划分的扩展八叉树注意力机制，这也可以通过张量的 reshape 和转置来高效实现。

## 3 Octree-Based Transformers

#### 3.0.1 Overview

1. 输入点云，归一化，转化成八叉树。每个八叉树的非空节点存储的特征是点的平均坐标、平均颜色、平均法向。
2. 转化八叉树的节点的存储特征，先降采样，随后变成高维向量。
3. 输入到 OctFormer 的编码器里面。
4. 下游任务。

### 3.1 Overview

|Symbols|Description|
|:-----:|:---------:|
| | |

#### 3.1.1 Attention







