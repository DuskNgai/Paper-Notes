# Perspective of Joint Distribution

记号同 [Variational Autoencoder](3%20Variational%20Autoencoder.md)。

## Maximizing Likelihood is Equivalent to Minimizing KL Divergence

对于训练集 $\mathcal{D}$，我们可能永远也不会知道其背后真正的分布 $\hat{p}(\mathbf{x})$，但是可以定义其**经验分布**为：
$$
e_{\mathcal{D}}(\mathbf{x}) = \frac{1}{N} \sum_{n = 1}^{N} \delta(\mathbf{x} - \mathbf{x}^{(n)})
$$
其中 $\delta(\cdot)$ 是 Dirac 函数。因此，模型分布 $p(\mathbf{x}; \theta)$ 与经验分布的 KL 散度为：
$$
\begin{aligned}
\mathop{\mathrm{KL}}[e_{\mathcal{D}}(\mathbf{x}) \parallel p(\mathbf{x}; \theta)] 
&= \mathbb{E}_{\mathbf{x} \sim e_{\mathcal{D}}(\mathbf{x})} \left[\log \frac{e_{\mathcal{D}}(\mathbf{x})}{p(\mathbf{x}; \theta)}\right] \\
&= \mathbb{E}_{\mathbf{x} \sim e_{\mathcal{D}}(\mathbf{x})} [\log e_{\mathcal{D}}(\mathbf{x})] - \mathbb{E}_{\mathbf{x} \sim e_{\mathcal{D}}(\mathbf{x})} [\log p(\mathbf{x}; \theta)] \\
&= -\mathcal{H}(e_{\mathcal{D}}(\mathbf{x})) - \frac{1}{N} \log p(\mathcal{D}; \theta)
\end{aligned}
$$
其中 $\mathcal{H}(p(\mathbf{x})) = -\int p(\mathbf{x}) \log p(\mathbf{x}) \mathrm{d}\mathbf{x}$ 是分布熵，且是常数。我们有：
$$
\boxed{
\mathop{\arg\min}_{\theta} \mathop{\mathrm{KL}}[e_{\mathcal{D}}(\mathbf{x}) \parallel p(\mathbf{x}; \theta)] = \mathop{\arg\max}_{\theta} \log p(\mathcal{D}; \theta)
}
$$
即**最小化经验分布 $e_{\mathcal{D}}(\mathbf{x})$ 与模型分布 $p(\mathbf{x}; \theta)$ 之间的 KL 散度等价于最大化对数似然 $\log p(\mathcal{D}; \theta)$**。

## Maximizing ELBO is Equivalent to Minimizing KL Divergence

当引入隐变量 $\mathbf{z}$ 时，我们定义数据和隐变量的**联合经验分布**：
$$
e_{\mathcal{D}}(\mathbf{x}, \mathbf{z}; \phi) = e_{\mathcal{D}}(\mathbf{x}) q(\mathbf{z} \mid \mathbf{x}; \phi)
$$
其中 $q(\mathbf{z} \mid \mathbf{x}; \phi)$ 是变分分布（近似后验）。显然模型分布也要跟着建模为联合分布，即 $p(\mathbf{x}, \mathbf{z}; \theta)$。

### Decomposition from ELBO

从 ELBO 的角度来分解联合分布的 KL 散度为：
$$
\begin{aligned}
\,&\mathop{\mathrm{KL}}[e_{\mathcal{D}}(\mathbf{x}, \mathbf{z}; \phi) \parallel p(\mathbf{x}, \mathbf{z}; \theta)] \\
=\,& \mathbb{E}_{\mathbf{x} \sim e_{\mathcal{D}}(\mathbf{x})} \left[\mathbb{E}_{\mathbf{z} \sim q(\mathbf{z} \mid \mathbf{x}; \phi)} \left[\log \frac{e_{\mathcal{D}}(\mathbf{x}) q(\mathbf{z} \mid \mathbf{x}; \phi)}{p(\mathbf{x}, \mathbf{z}; \theta)}\right]\right] \\
=\,& - \mathcal{H}(e_{\mathcal{D}}(\mathbf{x})) - \mathbb{E}_{\mathbf{x} \sim e_{\mathcal{D}}(\mathbf{x})} \left[\mathop{\mathrm{ELBO}}(q, \mathbf{x}; \theta, \phi)\right] \\
=\,& - \mathcal{H}(e_{\mathcal{D}}(\mathbf{x})) - \frac{1}{N} \mathop{\mathrm{ELBO}}(q, \mathcal{D}; \theta, \phi)
\end{aligned}
$$
因此得到关键等价关系：
$$
\boxed{
\mathop{\arg\min}_{\theta, \phi} \mathop{\mathrm{KL}}[e_{\mathcal{D}}(\mathbf{x}, \mathbf{z}; \phi) \parallel p(\mathbf{x}, \mathbf{z}; \theta)] = \mathop{\arg\max}_{\theta, \phi} \mathop{\mathrm{ELBO}}(q, \mathcal{D}; \theta, \phi)
}
$$

这表明**最小化联合分布的 KL 散度等价于最大化 ELBO**。这为MLE、EM、VAE 甚至 Diffusion Model 提供了统一框架。从这一角度来说，最小化 KL 散度是这类问题的本质目标。

### Decomposition from KL Divergence

从 KL 散度的角度来分解联合分布之间的 KL 散度为：
$$
\begin{aligned}
\,&\mathop{\mathrm{KL}}[e_{\mathcal{D}}(\mathbf{x}, \mathbf{z}; \phi) \parallel p(\mathbf{x}, \mathbf{z}; \theta)] \\
=\,& \mathbb{E}_{\mathbf{x} \sim e_{\mathcal{D}}(\mathbf{x})} \left[\mathbb{E}_{\mathbf{z} \sim q(\mathbf{z} \mid \mathbf{x}; \phi)} \left[\log \frac{e_{\mathcal{D}}(\mathbf{x}) q(\mathbf{z} \mid \mathbf{x}; \phi)}{p(\mathbf{x}; \theta) p(\mathbf{z} \mid \mathbf{x}; \theta)}\right]\right] \\
=\,& \mathop{\mathrm{KL}}[e_{\mathcal{D}}(\mathbf{x}) \parallel p(\mathbf{x}; \theta)] + \mathbb{E}_{\mathbf{x} \sim e_{\mathcal{D}}(\mathbf{x})} \left[\mathop{\mathrm{KL}}[q(\mathbf{z} \mid \mathbf{x}; \phi) \parallel p(\mathbf{z} \mid \mathbf{x}; \theta)]\right] \\
\ge\,& \mathop{\mathrm{KL}}[e_{\mathcal{D}}(\mathbf{x}) \parallel p(\mathbf{x}; \theta)]
\end{aligned}
$$
这表明**带隐变量的分布之间的 KL 散度是不带隐变量的 KL 散度的上界**。二者之间的差异就是由变分分布 $q(\mathbf{z} \mid \mathbf{x}; \phi)$ 和后验分布 $p(\mathbf{z} \mid \mathbf{x}; \theta)$ 之间的 KL 散度决定的。把联合分布的 KL 散度的表达式带入，我们能够还原 EM 算法中重要的关系式：
$$
\log p(\mathbf{x}; \theta) = \mathop{\mathrm{ELBO}}(q, \mathbf{x}; \theta, \phi) + \mathop{\mathrm{KL}}[q(\mathbf{z} \mid \mathbf{x}; \phi) \parallel p(\mathbf{z} \mid \mathbf{x}; \theta)]
$$
